{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "from apis.pc_apis import XHS_Apis\n",
    "from xhs_utils.common_utils import init\n",
    "from xhs_utils.data_util import handle_note_info, download_note, save_to_xlsx, timestamp_to_str\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import logging\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "# logging as before…\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Spider():\n",
    "    def __init__(self):\n",
    "        self.xhs_apis = XHS_Apis()\n",
    "\n",
    "    def spider_search_note_all_comment(self, note_info: dict, cookies_str: str, proxies: dict = None):\n",
    "        \"\"\"\n",
    "            Crawling all comments of a note\n",
    "            :param note_info: Note information\n",
    "            :param cookies_str: Cookies string\n",
    "            :param proxies: Proxy settings\n",
    "            :return: success, msg, list_all_comment\n",
    "        \"\"\"\n",
    "        list_all_comment = []\n",
    "        try:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            note_url = note_info[\"note_url\"]\n",
    "            success, msg, list_all_comment = self.xhs_apis.get_note_all_comment(note_url, cookies_str, proxies)\n",
    "            if not success:\n",
    "                raise Exception(msg)\n",
    "        except Exception as e:\n",
    "            success = False\n",
    "            msg = e\n",
    "        logger.info(f'Retrieving all comments - {note_url}: {success}, msg: {msg}')\n",
    "        return success, msg, list_all_comment\n",
    "    \n",
    "    def spider_note(self, note_url: str, cookies_str: str, proxies=None):\n",
    "        \"\"\"\n",
    "            爬取一个笔记的信息\n",
    "            :param note_url:\n",
    "            :param cookies_str:\n",
    "            :return:\n",
    "        \"\"\"\n",
    "        note_info = None\n",
    "        try:\n",
    "            success, msg, note_info = self.xhs_apis.get_note_info(note_url, cookies_str, proxies)\n",
    "            if success:\n",
    "                note_info = note_info['data']['items'][0]\n",
    "                note_info['url'] = note_url\n",
    "                note_info = handle_note_info(note_info)\n",
    "        except Exception as e:\n",
    "            success = False\n",
    "            msg = e\n",
    "        logger.info(f'Retrieving note info - {note_url}: {success}, msg: {msg}')\n",
    "\n",
    "        try:\n",
    "            # Add on comments in note_info (think like scrape 1 note_info and its all comments in next steps)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            success, msg, list_all_comment = self.spider_search_note_all_comment(note_info, cookies_str, proxies)\n",
    "            if success:\n",
    "                note_info['comments'] = list_all_comment\n",
    "            else:\n",
    "                note_info['comments'] = []\n",
    "        except Exception as e:\n",
    "            success = False\n",
    "            msg = e\n",
    "        return success, msg, note_info\n",
    "\n",
    "    def spider_some_note(self, list_notes_url: list, query:str, cookies_str: str, proxies=None):\n",
    "        \"\"\"\n",
    "        爬取一些笔记的信息\n",
    "        :param list_notes:\n",
    "        :param cookies_str:\n",
    "        :param base_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        list_note_info = []\n",
    "        outer_pbar = tqdm(total=len(list_notes_url), desc=f\"Processing note_info on keyword '{query}'\", unit=\"note_info\", leave=False)\n",
    "        for note_url in list_notes_url:\n",
    "            success, msg, note_info = self.spider_note(note_url, cookies_str, proxies)\n",
    "            if note_info is not None and success:\n",
    "                list_note_info.append(note_info)\n",
    "            else:\n",
    "                logger.error(f\"Failed to retrieve note info: {msg}\")\n",
    "            outer_pbar.update(1)\n",
    "        outer_pbar.close()\n",
    "        logger.info(f'Processed {len(list_notes_url)} notes.')\n",
    "        return list_note_info\n",
    "\n",
    "    def spider_some_search_note(self, query: str, require_num: int, cookies_str: str, sort=\"general\", proxies=None, set_url=None):\n",
    "        \"\"\"\n",
    "            Search and crawl notes, according to the keyword and number of notes\n",
    "            Args:\n",
    "                query (str): search keyword\n",
    "                require_num (int): number of notes to be crawled\n",
    "                cookies_str (str): Cookies\n",
    "                sort (str, optional): general\n",
    "                proxies (dict | None, optional): proxies settings, to avoid IP blockage\n",
    "                set_url (set | None, optional): To remove duplicate URLs\n",
    "\n",
    "            Returns:\n",
    "                tuple[bool, str, list[dict]]: (success, msg, list_note_info)\n",
    "        \"\"\"\n",
    "        time.sleep(3)\n",
    "        list_note_url = []\n",
    "        max_retries = 3\n",
    "        attempt = 1\n",
    "        while attempt <= max_retries:\n",
    "            try:\n",
    "                success, msg, list_notes = self.xhs_apis.search_some_note(query, require_num, cookies_str, sort, proxies)\n",
    "                if success:\n",
    "                    list_notes = list(filter(lambda x: x['model_type'] == \"note\", list_notes))\n",
    "                    logger.info(f'Searching keyword \"{query}\", note count: {len(list_notes)}')\n",
    "                    for note in list_notes:\n",
    "                        note_url = f\"https://www.xiaohongshu.com/explore/{note['id']}?xsec_token={note['xsec_token']}\"\n",
    "                        if isinstance(set_url, set):\n",
    "                            if note_url in set_url:\n",
    "                                continue\n",
    "                            set_url.add(note_url)\n",
    "                        list_note_url.append(note_url)\n",
    "                list_note_info = self.spider_some_note(list_note_url, query, cookies_str, proxies)\n",
    "                logger.info(f'Searching keyword \"{query}\", note: {success}, msg: {msg}')\n",
    "                return success, msg, list_note_info\n",
    "            \n",
    "            except Exception as e:\n",
    "                success = False\n",
    "                msg = e\n",
    "                attempt += 1\n",
    "                logger.warning(f\"Attempt {attempt} failed: {msg}\")\n",
    "                time.sleep(2 ** attempt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten helper\n",
    "# def handle_list_comment(note_comment):\n",
    "#     list_dict_comment = []\n",
    "#     list_dict_subcomment = []\n",
    "#     for comment in note_comment:\n",
    "#         list_dict_comment.append(comment)\n",
    "#         if \"sub_comments\" in comment:\n",
    "#             for sub in comment[\"sub_comments\"]:\n",
    "#                 sub[\"comment_id\"] = comment[\"id\"]\n",
    "#                 list_dict_subcomment.append(sub)\n",
    "#     return list_dict_comment, list_dict_subcomment\n",
    "\n",
    "# # Retry boilerplate\n",
    "# MAX_RETRIES = 3\n",
    "# BACKOFF_BASE = 2\n",
    "\n",
    "# def with_retry(fn):\n",
    "#     def wrapped(*args, **kwargs):\n",
    "#         for attempt in range(1, MAX_RETRIES + 1):\n",
    "#             try:\n",
    "#                 return fn(*args, **kwargs)\n",
    "#             except Exception as e:\n",
    "#                 if attempt < MAX_RETRIES:\n",
    "#                     delay = BACKOFF_BASE ** attempt + random.random()\n",
    "#                     logger.warning(f\"[{fn.__name__}] attempt {attempt} failed: {e!r}. retrying in {delay:.1f}s\")\n",
    "#                     time.sleep(delay)\n",
    "#                 else:\n",
    "#                     logger.error(f\"[{fn.__name__}] all {MAX_RETRIES} attempts failed: {e!r}\")\n",
    "#                     raise\n",
    "#     return wrapped\n",
    "\n",
    "# @with_retry\n",
    "# def delayed_spider_search_note(keyword, cookies_str, xhs_spider, num_search, set_url):\n",
    "#     # fixed 7s pre-delay\n",
    "#     time.sleep(random.uniform(4, 7))\n",
    "#     success, msg, notes = xhs_spider.spider_some_search_note(keyword, num_search, cookies_str, set_url=set_url)\n",
    "#     if not success:\n",
    "#         raise RuntimeError(f\"API returned success=False, msg={msg!r}\")\n",
    "#     # fixed 7s post-delay\n",
    "#     time.sleep(random.uniform(4, 7))\n",
    "#     return notes\n",
    "\n",
    "# @with_retry\n",
    "# def delayed_spider_search_comments(note_info, cookies_str, xhs_spider):\n",
    "#     # fixed 10s pre-delay\n",
    "#     time.sleep(random.uniform(4, 7))\n",
    "#     success, msg, comments = xhs_spider.spider_search_note_all_comment(note_info, cookies_str)\n",
    "#     if not success:\n",
    "#         raise RuntimeError(f\"API returned success=False, msg={msg!r}\")\n",
    "#     # fixed 10s post-delay\n",
    "#     time.sleep(random.uniform(4, 7))\n",
    "#     return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XHS Note scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhs_utils.common_utils import init\n",
    "\n",
    "cookies_str=\"abRequestId=b09f6390-3782-5b9a-8a12-527192621d8d; webBuild=4.62.3; a1=1963764c6d7ib7qqfcmnh47ilgwy6mmrhrbosup1830000356016; webId=841fd19f1e873a1b805ec456c22d6b01; gid=yjKqWK4i2YxqyjKqWK4SKF7UfW3DWAAiS6IxDyY4W31kElq8vK66FM888q2K8yK8dYJyqK8q; xsecappid=xhs-pc-web; acw_tc=0a4a88f517459845072174917e3606078ee10324bf2fa004970c47f5c17cd5; websectiga=2a3d3ea002e7d92b5c9743590ebd24010cf3710ff3af8029153751e41a6af4a3; sec_poison_id=1911d4df-e6d5-4311-b50c-2984efb14d91; web_session=040069b68989bc79b3e637d4243a4b4eaae933; unread={%22ub%22:%2267fb28f0000000001202c922%22%2C%22ue%22:%2267f641ec000000001c0304f0%22%2C%22uc%22:33}\"\n",
    "def trans_cookies(cookies_str):\n",
    "    if '; ' in cookies_str:\n",
    "        ck = {i.split('=')[0]: '='.join(i.split('=')[1:]) for i in cookies_str.split('; ')}\n",
    "    else:\n",
    "        ck = {i.split('=')[0]: '='.join(i.split('=')[1:]) for i in cookies_str.split(';')}\n",
    "    return ck\n",
    "\n",
    "def reverse_trans_cookies(cookies_dict):\n",
    "    return '; '.join([f'{k}={v}' for k, v in cookies_dict.items()])\n",
    "\n",
    "cookies_dict = {\n",
    "    'a1': '196374a2390wm4erg4g8raaupcvf5iyfq8gnp3kqy30000144617',\n",
    "    'web_session': '040069b96c5aba102af269522d3a4b34ec0697'\n",
    "}\n",
    "cookies_str = reverse_trans_cookies(cookies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Searching keyword \"ifast global bank\", note count: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38b64023dbc4987b16349463247a3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing note_info on keyword 'ifast global bank':   0%|          | 0/3 [00:00<?, ?note_info/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving note info - https://www.xiaohongshu.com/explore/681858d0000000002202edb1?xsec_token=AB0IsqDuvVKfeix1TVyEZyJ3nQQgEzN9dqQehkuFJM-EM=: True, msg: success\n",
      "INFO:__main__:Retrieving all comments - https://www.xiaohongshu.com/explore/681858d0000000002202edb1?xsec_token=AB0IsqDuvVKfeix1TVyEZyJ3nQQgEzN9dqQehkuFJM-EM=: True, msg: success\n",
      "INFO:__main__:Retrieving note info - https://www.xiaohongshu.com/explore/65fc0a27000000001203d8b4?xsec_token=ABzmyuLp8zek497CohhVbRO0vL2LFuo07F8MjhXopGmfU=: True, msg: success\n",
      "INFO:__main__:Retrieving all comments - https://www.xiaohongshu.com/explore/65fc0a27000000001203d8b4?xsec_token=ABzmyuLp8zek497CohhVbRO0vL2LFuo07F8MjhXopGmfU=: True, msg: success\n",
      "INFO:__main__:Retrieving note info - https://www.xiaohongshu.com/explore/67e29d0a000000000703645e?xsec_token=AB3Ky9JehAYxfqKlRugirr9npRQslw-WQaV3AlSe9ceNc=: True, msg: success\n",
      "INFO:__main__:Retrieving all comments - https://www.xiaohongshu.com/explore/67e29d0a000000000703645e?xsec_token=AB3Ky9JehAYxfqKlRugirr9npRQslw-WQaV3AlSe9ceNc=: True, msg: success\n",
      "INFO:__main__:Processed 3 notes.\n",
      "INFO:__main__:Searching keyword \"ifast global bank\", note: True, msg: success\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.xiaohongshu.com/explore/67e30ae1000000000903b3b7?xsec_token=ABLXM1oEHnZmA6xiBLs4CDB8iResK8T1E-0-xHTZdbrig=&xsec_source=pc_search&source=web_explore_feed\"\n",
    "\n",
    "\n",
    "def reverse_trans_cookies(cookies_dict):\n",
    "    return '; '.join([f'{k}={v}' for k, v in cookies_dict.items()])\n",
    "\n",
    "cookies_dict = {\n",
    "    'a1': '196374a2390wm4erg4g8raaupcvf5iyfq8gnp3kqy30000144617',\n",
    "    'web_session': '040069b96c5aba102af269522d3a4b34ec0697'\n",
    "}\n",
    "cookies_str = reverse_trans_cookies(cookies_dict)\n",
    "\n",
    "xhs_api = Data_Spider()\n",
    "set_url = set()\n",
    "data = xhs_api.spider_some_search_note(\"ifast global bank\", 20, cookies_str, set_url=set_url)\n",
    "# data = xhs_api.spider_search_note_all_comment(\"ifast global bank\", cookies_str, xhs_api, 3, set_url=set_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Searching keyword \"ifast global bank\", note count: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c9d6772cad4b06b1152f6db8014f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing note_info on keyword 'ifast global bank': 0note_info [00:00, ?note_info/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processed 0 notes.\n",
      "INFO:__main__:Searching keyword \"ifast global bank\", note: True, msg: success\n",
      "ERROR:__main__:[ERROR] Note search for 'ifast global bank' aborted: 'bool' object does not support item assignment\n",
      "INFO:__main__:Searching keyword \"ifast gb\", note count: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3635800ba7604b6bb1f2a210d5e68cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing note_info on keyword 'ifast gb': 0note_info [00:00, ?note_info/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processed 0 notes.\n",
      "INFO:__main__:Searching keyword \"ifast gb\", note: True, msg: success\n",
      "ERROR:__main__:[ERROR] Note search for 'ifast gb' aborted: 'bool' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Initialize\n",
    "    xhs_spider = Data_Spider()\n",
    "    cookies_str, base_path = init()\n",
    "\n",
    "    list_keywords = [\n",
    "        \"ifast global bank\", \"ifast gb\", #\"ifast debit card\", \"ifast银行\", \"ifast借记卡\",\n",
    "        #  \"ifast cashback\", \"奕丰环球银行\", \"ifast开户\", \"ifast入金\", \"ifast返现\"\n",
    "    ]\n",
    "    num_search = 3\n",
    "    list_search_note = []\n",
    "    set_note_url = set()\n",
    "    # === Sequential note searches ===\n",
    "    for kw in list_keywords:\n",
    "        try:\n",
    "            list_note_info_per_keyword = xhs_api.spider_some_search_note(kw, num_search, cookies_str, set_url=set_note_url)\n",
    "            for note_info in list_note_info_per_keyword:\n",
    "                note_info[\"keyword\"] = kw\n",
    "            list_search_note.extend(list_note_info_per_keyword)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] Note search for '{kw}' aborted: {e}\")\n",
    "\n",
    "\n",
    "    # Define folder_path \n",
    "    folder_path = \"datas/parquet_datas\"\n",
    "\n",
    "    # Note info dataframe\n",
    "    schema_name = \"mimir_igb_dpb_external_data.igb_xhs_scrape_data\"\n",
    "    df_pandas_note_info = pd.DataFrame(list_search_note)\n",
    "    # df_pandas_note_info = df_pandas_note_info.drop(columns=[\"avatar\"])\n",
    "    df_pandas_note_info = df_pandas_note_info.rename(columns={\"upload_time\": \"create_time\", \"nickname\": \"author_nickname\", \"user_url\": \"author_url\"})\n",
    "    df_pandas_note_info.to_parquet(f\"{folder_path}/xhs_api_note_info.parquet\", engine=\"pyarrow\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "AOAI_API_TYPE = os.getenv(\"AOAI_API_TYPE\")\n",
    "AOAI_API_VERSION = os.getenv(\"AOAI_API_VERSION\")\n",
    "AOAI_API_KEY = os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_API_ENDPOINT = os.getenv(\"AOAI_API_ENDPOINT\")\n",
    "AOAI_DEPLOYMENT_NAME = os.getenv(\"AOAI_DEPLOYMENT_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=AOAI_API_KEY,\n",
    "    api_version=AOAI_API_VERSION,\n",
    "    azure_endpoint=AOAI_API_ENDPOINT,\n",
    "    azure_deployment=AOAI_DEPLOYMENT_NAME\n",
    ")\n",
    "\n",
    "class RelevancyScoreOutput(BaseModel):\n",
    "    relevancy_score: float\n",
    "    relevancy_reasoning: str\n",
    "\n",
    "class BatchRelevancyOutput(BaseModel):\n",
    "    results: List[RelevancyScoreOutput]\n",
    "\n",
    "def evaluate_relevance_batch(posts: List[dict], batch_size: int = 10)-> List[RelevancyScoreOutput]:\n",
    "    system_prompt = \"\"\"\n",
    "You are a bilingual Chinese–English language model focused on identifying relevance to iFAST's financial services.\n",
    "\n",
    "Each post includes:\n",
    "    - keywords\n",
    "    - hashtags\n",
    "    - title\n",
    "    - description\n",
    "\n",
    "Your task:\n",
    "\t- Ignore posts about jobs, careers, interviews, or working at iFAST (score = 0.00)\n",
    "    - For all others, assess relevance to iFAST’s offerings, including:\n",
    "    - Account opening or management\n",
    "    - Investment products/platforms\n",
    "    - Digital banking & financial services\n",
    "    - Currency, cash management, or customer service\n",
    "\n",
    "Evaluate using:\n",
    "\t1.\tKeywords and hashtags\n",
    "\t2.\tSemantic match of title + description\n",
    "\n",
    "For each post:\n",
    "\t1.\tAssign a relevancy_score (0.00–1.00, two decimals)\n",
    "\t2.\tProvide a brief relevancy_reasoning (1–2 sentences)\n",
    "\n",
    "\"\"\"\n",
    "    all_outputs: List[RelevancyScoreOutput] = []\n",
    "\n",
    "    for i in range(0, len(posts), batch_size):\n",
    "        batch = posts[i : i + batch_size]\n",
    "        payload = json.dumps({\"posts\": batch}, ensure_ascii=False)\n",
    "\n",
    "        # Use the .parse endpoint to get Pydantic parsing\n",
    "        resp = client.beta.chat.completions.parse(\n",
    "            model=AOAI_DEPLOYMENT_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\",  \"content\": system_prompt},\n",
    "                {\"role\": \"user\",    \"content\": payload}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=2000,\n",
    "            response_format=BatchRelevancyOutput\n",
    "        )\n",
    "\n",
    "        # resp.choices[0].message.parsed will be a BatchRelevancyOutput\n",
    "        batch_output: BatchRelevancyOutput = resp.choices[0].message.parsed\n",
    "        all_outputs.extend(batch_output.results)\n",
    "\n",
    "    return all_outputs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    posts = [\n",
    "        {\n",
    "            \"keywords\":    row[\"keyword\"],\n",
    "            \"hashtag\":     row[\"tags\"],\n",
    "            \"title\":       row[\"title\"],\n",
    "            \"description\": row[\"desc\"]\n",
    "        }\n",
    "        for _, row in df_pandas_note_info.iterrows()\n",
    "    ]\n",
    "\n",
    "    outputs = evaluate_relevance_batch(posts, batch_size=10)\n",
    "\n",
    "    df_pandas_note_info_output = df_pandas_note_info.copy()\n",
    "\n",
    "    df_pandas_note_info_output = pd.concat([\n",
    "        df_pandas_note_info.reset_index(drop=True),\n",
    "        pd.DataFrame([o.dict() for o in outputs])\n",
    "    ], axis=1)\n",
    "\n",
    "    df_pandas_note_info_output.to_csv(\"test.csv\", encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_note_info_output = pd.concat([\n",
    "    df_pandas_note_info.reset_index(drop=True),\n",
    "    pd.DataFrame([o.dict() for o in outputs])\n",
    "], axis=1)\n",
    "\n",
    "df_pandas_note_info_output = df_pandas_note_info_output[df_pandas_note_info_output[\"relevancy_score\"] >= 0.6]\n",
    "\n",
    "df_pandas_note_info_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XHS Note comment scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/enghui.lau/Documents/Work/igb_scrape/xhs_spider/datas/parquet_datas/xhs_api_note_info.parquet\")\n",
    "\n",
    "len(df[df[\"comment_count\"].astype(int) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    list_search_comment = []\n",
    "    list_search_subcomment = []\n",
    "\n",
    "    # === Sequential comment fetches ===\n",
    "    for note in list_search_note:\n",
    "        # cookie = next(cookie_cycle_comments)\n",
    "        try:\n",
    "            raw_comments = delayed_spider_search_comments(note, cookies_str, xhs_spider)\n",
    "            comments, subcomments = handle_list_comment(raw_comments)\n",
    "            list_search_comment.extend(comments)\n",
    "            list_search_subcomment.extend(subcomments)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] Comments fetch aborted for note ID {note.get('id')}: {e}\")\n",
    "\n",
    "\n",
    "    # Main Comment dataframe\n",
    "    df_pandas_main_comment = pd.DataFrame(list_search_comment)\n",
    "    df_pandas_main_comment = df_pandas_main_comment.drop(columns=[\"pictures\", \"user_info\", \"at_users\", \"status\", \"sub_comment_cursor\", \"sub_comment_has_more\", \"show_tags\", \"sub_comments\"])\n",
    "    df_pandas_main_comment[\"create_time\"] = df_pandas_main_comment[\"create_time\"].apply(lambda x: timestamp_to_str(x))\n",
    "    df_pandas_main_comment = df_pandas_main_comment.rename(columns={\"comment_id\": \"main_comment_id\", \"content\": \"main_comment\"})\n",
    "    df_pandas_main_comment.to_parquet(f\"{folder_path}/xhs_api_main_comment.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "    # Sub Comment dataframe\n",
    "    df_pandas_sub_comment = pd.DataFrame(list_search_subcomment)\n",
    "    df_pandas_sub_comment = df_pandas_sub_comment.drop(columns=[\"status\", \"user_info\", \"show_tags\", \"target_comment\", \"at_users\", \"pictures\"])\n",
    "    df_pandas_sub_comment[\"create_time\"] = df_pandas_sub_comment[\"create_time\"].apply(lambda x: timestamp_to_str(x))\n",
    "    df_pandas_sub_comment = df_pandas_sub_comment.rename(columns={\"comment_id\": \"main_comment_id\", \"content\": \"sub_comment\"})\n",
    "    df_pandas_sub_comment.to_parquet(f\"{folder_path}/xhs_api_sub_comment.parquet\", engine=\"pyarrow\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
